# -*- coding: utf-8 -*-
"""Mandibula x Idade x IA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JWc_RlEADDf4kIAvKJaJ5NqnVI0L2zCT

**Data preparation**
"""

import pandas as pd
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
import matplotlib.pyplot as plt
import numpy as np

file_path = '/content/dataset.xlsx'
df = pd.read_excel(file_path)

df.columns

colunas = ['IDADE CRONOLÓGICA','Gender','Co-Go (mm)', 'Go-Gn (mm)', 'Cond-Pog (mm)',
       'Ar-Go-Me (°)']

df = df.loc[:, colunas]

coluna_grupo = df.pop('IDADE CRONOLÓGICA')
df.insert(0, 'IDADE CRONOLÓGICA', coluna_grupo)

df = df.dropna()

df.info()

df.to_excel('estatistica.xlsx', index=False)

"""**Model building**"""

X = df.iloc[:,1:]
y = df.iloc[:,0]

# Outliers detection
Q1 = np.percentile(y, 25)
Q3 = np.percentile(y, 75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = y[(y < lower_bound) | (y > upper_bound)]

print("Outliers:", outliers)

#RANDOM STATE
RANDOM_STATE = 42

#Dataset split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=RANDOM_STATE, shuffle=True)

##Data normalization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**GRADIENT BOOSTING REGRESSOR**"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de Gradient Boosting
gb_model = GradientBoostingRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_gb = grid_search.best_params_

# Treinar o melhor modelo encontrado
best_gb_model_cl = GradientBoostingRegressor(**best_params_gb)

# Ajustar o modelo nos dados de treino
best_gb_model_cl.fit(X_train, y_train)
y_pred_test_gb2 = best_gb_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_gb = mean_squared_error(y_test, y_pred_test_gb2)
mae_gb = mean_absolute_error(y_test, y_pred_test_gb2)
rmse_gb = sqrt(mse_gb)
r2_gb = r2_score(y_test, y_pred_test_gb2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_gb = bootstrap_metrics_test(best_gb_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_gb = ci95(metrics_test_gb['mse'])
rmse_ci_test_gb = ci95(metrics_test_gb['rmse'])
mae_ci_test_gb = ci95(metrics_test_gb['mae'])
r2_ci_test_gb = ci95(metrics_test_gb['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_gb['mse'])} (95% CI: {mse_ci_test_gb})")
print(f"Test Data RMSE: {np.mean(metrics_test_gb['rmse'])} (95% CI: {rmse_ci_test_gb})")
print(f"Test Data MAE: {np.mean(metrics_test_gb['mae'])} (95% CI: {mae_ci_test_gb})")
print(f"Test Data R2: {np.mean(metrics_test_gb['r2'])} (95% CI: {r2_ci_test_gb})")

# Realizando validação cruzada
kf_gb = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_gb2 = cross_val_predict(best_gb_model_cl, X_train, y_train, cv=kf_gb)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_gb = bootstrap_cv_metrics_regression(best_gb_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_gb = ci95(metrics_cv_gb['mse'])
rmse_ci_cv_gb = ci95(metrics_cv_gb['rmse'])
mae_ci_cv_gb = ci95(metrics_cv_gb['mae'])
r2_ci_cv_gb = ci95(metrics_cv_gb['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_gb['mse'])} (95% CI: {mse_ci_cv_gb})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_gb['rmse'])} (95% CI: {rmse_ci_cv_gb})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_gb['mae'])} (95% CI: {mae_ci_cv_gb})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_gb['r2'])} (95% CI: {r2_ci_cv_gb})")

##FEATURE IMPORTANCE GRADIENT BOOSTING CLASSIFIER
X = df.drop('IDADE CRONOLÓGICA', axis=1)
features=[]
for columns in X.columns:
    features.append(columns)
imp_features = best_gb_model_cl.feature_importances_
importances = best_gb_model_cl.feature_importances_

feature_importance_gb = pd.DataFrame({'Feature': features, 'Importance': importances})

feature_importance_sorted = feature_importance_gb.sort_values('Importance', ascending=True)
colors = plt.cm.viridis(np.linspace(0.2, 1, len(feature_importance_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_sorted['Feature'], feature_importance_sorted['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()
plt.show()

"""**LINEAR REGRESSION**"""

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo de regressão
regression_model = LinearRegression()

# Definir a grade de hiperparâmetros
param_grid = {
    'fit_intercept': [True, False],
    'copy_X': [True, False],
    'n_jobs': [-1, 1],
    'positive': [True, False]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(regression_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_linear = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_linear_model_cl = LinearRegression(**best_params_linear)

# Ajustar o modelo nos dados de treino
best_linear_model_cl.fit(X_train, y_train)
y_pred_test_linear2 = best_linear_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_linear = mean_squared_error(y_test, y_pred_test_linear2)
mae_linear = mean_absolute_error(y_test, y_pred_test_linear2)
rmse_linear = sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_test_linear2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_linear = bootstrap_metrics_test(best_linear_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_linear = ci95(metrics_test_linear['mse'])
rmse_ci_test_linear = ci95(metrics_test_linear['rmse'])
mae_ci_test_linear = ci95(metrics_test_linear['mae'])
r2_ci_test_linear = ci95(metrics_test_linear['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_linear['mse'])} (95% CI: {mse_ci_test_linear})")
print(f"Test Data RMSE: {np.mean(metrics_test_linear['rmse'])} (95% CI: {rmse_ci_test_linear})")
print(f"Test Data MAE: {np.mean(metrics_test_linear['mae'])} (95% CI: {mae_ci_test_linear})")
print(f"Test Data R2: {np.mean(metrics_test_linear['r2'])} (95% CI: {r2_ci_test_linear})")

# Realizando validação cruzada
kf_linear = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_linear2 = cross_val_predict(best_linear_model_cl, X_train, y_train, cv=kf_linear)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_linear = bootstrap_cv_metrics_regression(best_linear_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_linear = ci95(metrics_cv_linear['mse'])
rmse_ci_cv_linear = ci95(metrics_cv_linear['rmse'])
mae_ci_cv_linear = ci95(metrics_cv_linear['mae'])
r2_ci_cv_linear = ci95(metrics_cv_linear['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_linear['mse'])} (95% CI: {mse_ci_cv_linear})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_linear['rmse'])} (95% CI: {rmse_ci_cv_linear})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_linear['mae'])} (95% CI: {mae_ci_cv_linear})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_linear['r2'])} (95% CI: {r2_ci_cv_linear})")

coefficients = best_linear_model_cl.coef_

feature_importance_lr = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance_lr = feature_importance_lr.sort_values('Importance', ascending=True)

colors = plt.cm.viridis(np.linspace(0.2, 1, len(feature_importance_lr)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_lr['Feature'], feature_importance_lr['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()
plt.show()

"""**SVM**"""

from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo SVM
svm_model = SVR()

# Definir a grade de hiperparâmetros
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'degree': [2, 3, 4]
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_svm = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_svm_model_cl = SVR(**best_params_svm)

# Ajustar o modelo nos dados de treino
best_svm_model_cl.fit(X_train, y_train)
y_pred_test_svm2 = best_svm_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_svm = mean_squared_error(y_test, y_pred_test_svm2)
mae_svm = mean_absolute_error(y_test, y_pred_test_svm2)
rmse_svm = sqrt(mse_svm)
r2_svm = r2_score(y_test, y_pred_test_svm2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_svm = bootstrap_metrics_test(best_svm_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_svm = ci95(metrics_test_svm['mse'])
rmse_ci_test_svm = ci95(metrics_test_svm['rmse'])
mae_ci_test_svm = ci95(metrics_test_svm['mae'])
r2_ci_test_svm = ci95(metrics_test_svm['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_svm['mse'])} (95% CI: {mse_ci_test_svm})")
print(f"Test Data RMSE: {np.mean(metrics_test_svm['rmse'])} (95% CI: {rmse_ci_test_svm})")
print(f"Test Data MAE: {np.mean(metrics_test_svm['mae'])} (95% CI: {mae_ci_test_svm})")
print(f"Test Data R2: {np.mean(metrics_test_svm['r2'])} (95% CI: {r2_ci_test_svm})")

# Realizando validação cruzada
kf_svm = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_svm2 = cross_val_predict(best_svm_model_cl, X_train, y_train, cv=kf_svm)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_svm = bootstrap_cv_metrics_regression(best_svm_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_svm = ci95(metrics_cv_svm['mse'])
rmse_ci_cv_svm = ci95(metrics_cv_svm['rmse'])
mae_ci_cv_svm = ci95(metrics_cv_svm['mae'])
r2_ci_cv_svm = ci95(metrics_cv_svm['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_svm['mse'])} (95% CI: {mse_ci_cv_svm})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_svm['rmse'])} (95% CI: {rmse_ci_cv_svm})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_svm['mae'])} (95% CI: {mae_ci_cv_svm})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_svm['r2'])} (95% CI: {r2_ci_cv_svm})")

"""**KNN**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo KNN
knn_model = KNeighborsRegressor()

# Definir a grade de hiperparâmetros
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # p=1 para distância de Manhattan, p=2 para distância Euclidiana
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_knn = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_knn_model_cl = KNeighborsRegressor(**best_params_knn)

# Ajustar o modelo nos dados de treino
best_knn_model_cl.fit(X_train, y_train)
y_pred_test_knn2 = best_knn_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_knn = mean_squared_error(y_test, y_pred_test_knn2)
mae_knn = mean_absolute_error(y_test, y_pred_test_knn2)
rmse_knn = sqrt(mse_knn)
r2_knn = r2_score(y_test, y_pred_test_knn2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_knn = bootstrap_metrics_test(best_knn_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_knn = ci95(metrics_test_knn['mse'])
rmse_ci_test_knn = ci95(metrics_test_knn['rmse'])
mae_ci_test_knn = ci95(metrics_test_knn['mae'])
r2_ci_test_knn = ci95(metrics_test_knn['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_knn['mse'])} (95% CI: {mse_ci_test_knn})")
print(f"Test Data RMSE: {np.mean(metrics_test_knn['rmse'])} (95% CI: {rmse_ci_test_knn})")
print(f"Test Data MAE: {np.mean(metrics_test_knn['mae'])} (95% CI: {mae_ci_test_knn})")
print(f"Test Data R2: {np.mean(metrics_test_knn['r2'])} (95% CI: {r2_ci_test_knn})")

# Realizando validação cruzada
kf_knn = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_knn2 = cross_val_predict(best_knn_model_cl, X_train, y_train, cv=kf_knn)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_knn = bootstrap_cv_metrics_regression(best_knn_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_knn = ci95(metrics_cv_knn['mse'])
rmse_ci_cv_knn = ci95(metrics_cv_knn['rmse'])
mae_ci_cv_knn = ci95(metrics_cv_knn['mae'])
r2_ci_cv_knn = ci95(metrics_cv_knn['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_knn['mse'])} (95% CI: {mse_ci_cv_knn})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_knn['rmse'])} (95% CI: {rmse_ci_cv_knn})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_knn['mae'])} (95% CI: {mae_ci_cv_knn})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_knn['r2'])} (95% CI: {r2_ci_cv_knn})")

"""**RANDOM FOREST**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Random Forest
rf_model = RandomForestRegressor(random_state=RANDOM_STATE)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [50, 100, 200],  # Número de árvores na floresta
    'max_depth': [None, 10, 20],  # Profundidade máxima das árvores
    'min_samples_split': [2, 5, 10],  # Número mínimo de amostras necessárias para dividir um nó interno
    'min_samples_leaf': [1, 2, 4],  # Número mínimo de amostras necessárias para ser uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # Número de features a serem consideradas para cada split
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_rf = grid_search.best_params_
print("Best Parameters:", grid_search.best_params_)
# Treinar o melhor modelo encontrado
best_rf_model_cl = RandomForestRegressor(**best_params_rf, random_state=42)

# Ajustar o modelo nos dados de treino
best_rf_model_cl.fit(X_train, y_train)
y_pred_test_rf2 = best_rf_model_cl.predict(X_test)

# Calcular os erros no conjunto de teste
mse_rf = mean_squared_error(y_test, y_pred_test_rf2)
mae_rf = mean_absolute_error(y_test, y_pred_test_rf2)
rmse_rf = sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_test_rf2)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_rf = bootstrap_metrics_test(best_rf_model_cl, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_rf = ci95(metrics_test_rf['mse'])
rmse_ci_test_rf = ci95(metrics_test_rf['rmse'])
mae_ci_test_rf = ci95(metrics_test_rf['mae'])
r2_ci_test_rf = ci95(metrics_test_rf['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_rf['mse'])} (95% CI: {mse_ci_test_rf})")
print(f"Test Data RMSE: {np.mean(metrics_test_rf['rmse'])} (95% CI: {rmse_ci_test_rf})")
print(f"Test Data MAE: {np.mean(metrics_test_rf['mae'])} (95% CI: {mae_ci_test_rf})")
print(f"Test Data R2: {np.mean(metrics_test_rf['r2'])} (95% CI: {r2_ci_test_rf})")

# Realizando validação cruzada
kf_rf = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_rf2 = cross_val_predict(best_rf_model_cl, X_train, y_train, cv=kf_rf)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_rf = bootstrap_cv_metrics_regression(best_rf_model_cl, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_rf = ci95(metrics_cv_rf['mse'])
rmse_ci_cv_rf = ci95(metrics_cv_rf['rmse'])
mae_ci_cv_rf = ci95(metrics_cv_rf['mae'])
r2_ci_cv_rf = ci95(metrics_cv_rf['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_rf['mse'])} (95% CI: {mse_ci_cv_rf})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_rf['rmse'])} (95% CI: {rmse_ci_cv_rf})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_rf['mae'])} (95% CI: {mae_ci_cv_rf})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_rf['r2'])} (95% CI: {r2_ci_cv_rf})")

X_3 = df.drop('IDADE CRONOLÓGICA', axis=1)
features_3 = []

for column in X_3.columns:
    features_3.append(column)

imp_features_dt3 = best_rf_model_cl.feature_importances_

df_imp_features_dt3 = pd.DataFrame({"features": features_3, "weights": imp_features_dt3})
df_imp_features_dt3_sorted = df_imp_features_dt3.sort_values(by='weights', ascending=True)

colors = plt.cm.viridis(np.linspace(0.2, 1, len(df_imp_features_dt3_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(df_imp_features_dt3_sorted['features'], df_imp_features_dt3_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""**Adaboost Regressor**"""

from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo AdaBoost
ada_model = AdaBoostRegressor(random_state=42)

# Definir a grade de hiperparâmetros
param_grid = {
    'n_estimators': [50, 100, 200],  # Número de estimadores base
    'learning_rate': [0.01, 0.1, 1.0],  # Taxa de aprendizado
    'loss': ['linear', 'square', 'exponential']  # Função de perda
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(ada_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_ada = grid_search.best_params_
print("Best Parameters:", best_params_ada)

# Treinar o melhor modelo encontrado
best_ada_model = AdaBoostRegressor(**best_params_ada, random_state=42)

# Ajustar o modelo nos dados de treino
best_ada_model.fit(X_train, y_train)
y_pred_test_ada = best_ada_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_ada = mean_squared_error(y_test, y_pred_test_ada)
mae_ada = mean_absolute_error(y_test, y_pred_test_ada)
rmse_ada = sqrt(mse_ada)
r2_ada = r2_score(y_test, y_pred_test_ada)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_ada = bootstrap_metrics_test(best_ada_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_ada = ci95(metrics_test_ada['mse'])
rmse_ci_test_ada = ci95(metrics_test_ada['rmse'])
mae_ci_test_ada = ci95(metrics_test_ada['mae'])
r2_ci_test_ada = ci95(metrics_test_ada['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_ada['mse'])} (95% CI: {mse_ci_test_ada})")
print(f"Test Data RMSE: {np.mean(metrics_test_ada['rmse'])} (95% CI: {rmse_ci_test_ada})")
print(f"Test Data MAE: {np.mean(metrics_test_ada['mae'])} (95% CI: {mae_ci_test_ada})")
print(f"Test Data R2: {np.mean(metrics_test_ada['r2'])} (95% CI: {r2_ci_test_ada})")

# Realizando validação cruzada
kf_ada = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_ada = cross_val_predict(best_ada_model, X_train, y_train, cv=kf_ada)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_ada = bootstrap_cv_metrics_regression(best_ada_model, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_ada = ci95(metrics_cv_ada['mse'])
rmse_ci_cv_ada = ci95(metrics_cv_ada['rmse'])
mae_ci_cv_ada = ci95(metrics_cv_ada['mae'])
r2_ci_cv_ada = ci95(metrics_cv_ada['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_ada['mse'])} (95% CI: {mse_ci_cv_ada})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_ada['rmse'])} (95% CI: {rmse_ci_cv_ada})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_ada['mae'])} (95% CI: {mae_ci_cv_ada})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_ada['r2'])} (95% CI: {r2_ci_cv_ada})")

# Certifique-se de que X_3 é o DataFrame sem a variável alvo (IDADE CRONOLÓGICA)
X_3 = df.drop('IDADE CRONOLÓGICA', axis=1)

# Obter os nomes das features
features_3 = X_3.columns.tolist()

# Obter a importância das features do modelo AdaBoost
imp_features_ada = best_ada_model.feature_importances_

# Criar um DataFrame para organizar as importâncias
df_imp_features_ada = pd.DataFrame({"features": features_3, "weights": imp_features_ada})
df_imp_features_ada_sorted = df_imp_features_ada.sort_values(by='weights', ascending=True)

# Criar o gráfico de barras horizontais
colors = plt.cm.viridis(np.linspace(0.2, 1, len(df_imp_features_ada_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(df_imp_features_ada_sorted['features'], df_imp_features_ada_sorted['weights'], color=colors)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.title('Feature Importance (AdaBoost)', fontsize=14, weight='bold')

# Adicionar valores das importâncias ao lado das barras
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

# Melhorar estética do gráfico
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

# Mostrar o gráfico
plt.show()

"""**Decision Tree**"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo Decision Tree
dt_model = DecisionTreeRegressor(random_state=42)

# Definir a grade de hiperparâmetros
param_grid = {
    'max_depth': [None, 10, 20, 30],  # Profundidade máxima da árvore
    'min_samples_split': [2, 5, 10],  # Número mínimo de amostras para dividir um nó
    'min_samples_leaf': [1, 2, 4],  # Número mínimo de amostras em uma folha
    'max_features': ['auto', 'sqrt', 'log2']  # Número de features a serem consideradas para cada split
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
best_params_dt = grid_search.best_params_
print("Best Parameters:", best_params_dt)

# Treinar o melhor modelo encontrado
best_dt_model = DecisionTreeRegressor(**best_params_dt, random_state=42)

# Ajustar o modelo nos dados de treino
best_dt_model.fit(X_train, y_train)
y_pred_test_dt = best_dt_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_dt = mean_squared_error(y_test, y_pred_test_dt)
mae_dt = mean_absolute_error(y_test, y_pred_test_dt)
rmse_dt = sqrt(mse_dt)
r2_dt = r2_score(y_test, y_pred_test_dt)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_dt = bootstrap_metrics_test(best_dt_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_dt = ci95(metrics_test_dt['mse'])
rmse_ci_test_dt = ci95(metrics_test_dt['rmse'])
mae_ci_test_dt = ci95(metrics_test_dt['mae'])
r2_ci_test_dt = ci95(metrics_test_dt['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_dt['mse'])} (95% CI: {mse_ci_test_dt})")
print(f"Test Data RMSE: {np.mean(metrics_test_dt['rmse'])} (95% CI: {rmse_ci_test_dt})")
print(f"Test Data MAE: {np.mean(metrics_test_dt['mae'])} (95% CI: {mae_ci_test_dt})")
print(f"Test Data R2: {np.mean(metrics_test_dt['r2'])} (95% CI: {r2_ci_test_dt})")

# Realizando validação cruzada
kf_dt = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_dt = cross_val_predict(best_dt_model, X_train, y_train, cv=kf_dt)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_dt = bootstrap_cv_metrics_regression(best_dt_model, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_dt = ci95(metrics_cv_dt['mse'])
rmse_ci_cv_dt = ci95(metrics_cv_dt['rmse'])
mae_ci_cv_dt = ci95(metrics_cv_dt['mae'])
r2_ci_cv_dt = ci95(metrics_cv_dt['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_dt['mse'])} (95% CI: {mse_ci_cv_dt})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_dt['rmse'])} (95% CI: {rmse_ci_cv_dt})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_dt['mae'])} (95% CI: {mae_ci_cv_dt})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_dt['r2'])} (95% CI: {r2_ci_cv_dt})")

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Certifique-se de que X_3 é o DataFrame sem a variável alvo (IDADE CRONOLÓGICA)
X_3 = df.drop('IDADE CRONOLÓGICA', axis=1)

# Obter os nomes das features
features_3 = X_3.columns.tolist()

# Obter a importância das features do modelo Decision Tree
imp_features_dt = best_dt_model.feature_importances_

# Criar um DataFrame para organizar as importâncias
df_imp_features_dt = pd.DataFrame({"features": features_3, "weights": imp_features_dt})
df_imp_features_dt_sorted = df_imp_features_dt.sort_values(by='weights', ascending=True)

# Criar o gráfico de barras horizontais
colors = plt.cm.viridis(np.linspace(0.2, 1, len(df_imp_features_dt_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(df_imp_features_dt_sorted['features'], df_imp_features_dt_sorted['weights'], color=colors)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.title('Feature Importance (Decision Tree)', fontsize=14, weight='bold')

# Adicionar valores das importâncias ao lado das barras
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

# Melhorar estética do gráfico
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

# Mostrar o gráfico
plt.show()

"""**MLP Regressor**"""

from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
import numpy as np
from sklearn.utils import resample

# Definir o modelo MLP
mlp_model = MLPRegressor(random_state=42, max_iter=5000)

# Definir a grade de hiperparâmetros
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Diferentes arquiteturas de camadas ocultas
    'activation': ['relu', 'tanh'],  # Funções de ativação
    'solver': ['adam', 'lbfgs'],  # Otimizadores
    'alpha': [0.0001, 0.001, 0.01],  # Regularização L2
    'learning_rate': ['constant', 'adaptive']  # Taxa de aprendizado
}

# Realizar a busca em grade com validação cruzada
grid_search = GridSearchCV(mlp_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)
best_params_mlp = grid_search.best_params_
print("Best Parameters:", best_params_mlp)

# Treinar o melhor modelo encontrado
best_mlp_model = MLPRegressor(**best_params_mlp, random_state=42, max_iter=5000)

# Ajustar o modelo nos dados de treino
best_mlp_model.fit(X_train, y_train)
y_pred_test_mlp = best_mlp_model.predict(X_test)

# Calcular os erros no conjunto de teste
mse_mlp = mean_squared_error(y_test, y_pred_test_mlp)
mae_mlp = mean_absolute_error(y_test, y_pred_test_mlp)
rmse_mlp = sqrt(mse_mlp)
r2_mlp = r2_score(y_test, y_pred_test_mlp)

# Função para calcular as métricas com bootstrapping
def bootstrap_metrics_test(model, X_test, y_test, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X_test, y_test)
        y_pred = model.predict(X_resampled)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred))
        metrics['r2'].append(r2_score(y_resampled, y_pred))
    return metrics

# Calcular as métricas de bootstrapping para os dados de teste
metrics_test_mlp = bootstrap_metrics_test(best_mlp_model, X_test, y_test)

# Calcular os IC95% para os dados de teste
def ci95(data):
    return np.percentile(data, [2.5, 97.5])

mse_ci_test_mlp = ci95(metrics_test_mlp['mse'])
rmse_ci_test_mlp = ci95(metrics_test_mlp['rmse'])
mae_ci_test_mlp = ci95(metrics_test_mlp['mae'])
r2_ci_test_mlp = ci95(metrics_test_mlp['r2'])

print(f"Test Data MSE: {np.mean(metrics_test_mlp['mse'])} (95% CI: {mse_ci_test_mlp})")
print(f"Test Data RMSE: {np.mean(metrics_test_mlp['rmse'])} (95% CI: {rmse_ci_test_mlp})")
print(f"Test Data MAE: {np.mean(metrics_test_mlp['mae'])} (95% CI: {mae_ci_test_mlp})")
print(f"Test Data R2: {np.mean(metrics_test_mlp['r2'])} (95% CI: {r2_ci_test_mlp})")

# Realizando validação cruzada
kf_mlp = KFold(n_splits=5, shuffle=True, random_state=42)
y_pred_cv_mlp = cross_val_predict(best_mlp_model, X_train, y_train, cv=kf_mlp)

# Função para calcular o IC95% na validação cruzada com bootstrapping
def bootstrap_cv_metrics_regression(model, X, y, n_iterations=1000, cv=5, random_state=42):
    np.random.seed(random_state)
    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mse': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)
        y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)
        metrics['mse'].append(mean_squared_error(y_resampled, y_pred_cv))
        metrics['rmse'].append(np.sqrt(mean_squared_error(y_resampled, y_pred_cv)))
        metrics['mae'].append(mean_absolute_error(y_resampled, y_pred_cv))
        metrics['r2'].append(r2_score(y_resampled, y_pred_cv))
    return metrics

# Calcular as métricas de bootstrapping para validação cruzada
metrics_cv_mlp = bootstrap_cv_metrics_regression(best_mlp_model, X_train, y_train, cv=5)

# Calcular os IC95% para validação cruzada
mse_ci_cv_mlp = ci95(metrics_cv_mlp['mse'])
rmse_ci_cv_mlp = ci95(metrics_cv_mlp['rmse'])
mae_ci_cv_mlp = ci95(metrics_cv_mlp['mae'])
r2_ci_cv_mlp = ci95(metrics_cv_mlp['r2'])

print(f"Cross-Validation MSE: {np.mean(metrics_cv_mlp['mse'])} (95% CI: {mse_ci_cv_mlp})")
print(f"Cross-Validation RMSE: {np.mean(metrics_cv_mlp['rmse'])} (95% CI: {rmse_ci_cv_mlp})")
print(f"Cross-Validation MAE: {np.mean(metrics_cv_mlp['mae'])} (95% CI: {mae_ci_cv_mlp})")
print(f"Cross-Validation R2: {np.mean(metrics_cv_mlp['r2'])} (95% CI: {r2_ci_cv_mlp})")

"""**SCATTER PLOT**"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def calculate_metrics_with_ci(y_true, y_pred, n_iterations=1000, random_state=42):
    """
    Calculates MAE, RMSE, and R² metrics along with 95% confidence intervals
    using bootstrap resampling.
    """
    np.random.seed(random_state)
    mae_list, rmse_list, r2_list = [], [], []

    for _ in range(n_iterations):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred_resampled = y_pred[indices]

        mae_list.append(mean_absolute_error(y_true_resampled, y_pred_resampled))
        rmse_list.append(np.sqrt(mean_squared_error(y_true_resampled, y_pred_resampled)))
        r2_list.append(r2_score(y_true_resampled, y_pred_resampled))

    metrics = {
        "MAE": (np.mean(mae_list), np.percentile(mae_list, [2.5, 97.5])),
        "RMSE": (np.mean(rmse_list), np.percentile(rmse_list, [2.5, 97.5])),
        "R²":  (np.mean(r2_list), np.percentile(r2_list, [2.5, 97.5]))
    }
    return metrics

# Example dictionary with models and predictions
models = {
    "Linear Regression": y_pred_test_linear2,
    "Gradient Boosting": y_pred_test_gb2,
    "SVR": y_pred_test_svm2,
    "KNeighbors": y_pred_test_knn2,
    "Random Forest": y_pred_test_rf2,
    "MLP": y_pred_test_mlp,
    "Decision Tree": y_pred_test_dt,
    "AdaBoost": y_pred_test_ada
}

y_test_array = np.array(y_test)

# Figure with generous size
fig, axs = plt.subplots(3, 3, figsize=(24, 18))
axs = axs.flatten()  # Flatten to easily iterate

for i, (model_name, y_pred) in enumerate(models.items()):
    y_pred_array = np.array(y_pred)

    # Calculate metrics and confidence intervals
    metrics = calculate_metrics_with_ci(y_test_array, y_pred_array)
    mae, mae_ci = metrics["MAE"]
    rmse, rmse_ci = metrics["RMSE"]
    r2, r2_ci = metrics["R²"]

    # Determine min and max for axes and identity line
    x_min, x_max = y_test_array.min(), y_test_array.max()
    y_min, y_max = y_pred_array.min(), y_pred_array.max()
    overall_min = min(x_min, y_min)
    overall_max = max(x_max, y_max)

    # Scatter plot
    axs[i].scatter(
        y_test_array,
        y_pred_array,
        color='royalblue',
        alpha=0.6,
        s=100,
        label='Predicted Values'
    )
    # Identity line
    axs[i].plot(
        [overall_min, overall_max],
        [overall_min, overall_max],
        'k--',
        label='y = x (Identity)'
    )

    axs[i].set_title(f"{model_name}: Actual vs Predicted", fontsize=14, pad=10)
    axs[i].set_xlabel("Actual Values", fontsize=12)
    axs[i].set_ylabel("Predicted Values", fontsize=12)
    axs[i].grid(True, alpha=0.3)

    # Set tight axis limits so data + identity line are fully visible
    axs[i].set_xlim([overall_min, overall_max])
    axs[i].set_ylim([overall_min, overall_max])

    # Place metrics in the bottom-right corner
    axs[i].text(
        0.95,
        0.05,
        f"MAE: {mae:.2f} [{mae_ci[0]:.2f}, {mae_ci[1]:.2f}]\n"
        f"RMSE: {rmse:.2f} [{rmse_ci[0]:.2f}, {rmse_ci[1]:.2f}]\n"
        f"R²: {r2:.2f} [{r2_ci[0]:.2f}, {r2_ci[1]:.2f}]",
        transform=axs[i].transAxes,
        fontsize=10,
        verticalalignment='bottom',
        horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.4', facecolor='ivory', alpha=0.8)
    )

    axs[i].legend(fontsize=10, loc='upper left')

# Remove extra subplots if fewer than 9 models
for j in range(len(models), len(axs)):
    fig.delaxes(axs[j])

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from scipy.stats import gaussian_kde
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def calculate_metrics_with_ci(y_true, y_pred, n_iterations=1000, random_state=42):
    """
    Calculates MAE, RMSE, and R² metrics along with 95% confidence intervals
    using bootstrap resampling.
    """
    np.random.seed(random_state)
    mae_list, rmse_list, r2_list = [], [], []

    for _ in range(n_iterations):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred_resampled = y_pred[indices]

        mae_list.append(mean_absolute_error(y_true_resampled, y_pred_resampled))
        rmse_list.append(np.sqrt(mean_squared_error(y_true_resampled, y_pred_resampled)))
        r2_list.append(r2_score(y_true_resampled, y_pred_resampled))

    metrics = {
        "MAE": (np.mean(mae_list), np.percentile(mae_list, [2.5, 97.5])),
        "RMSE": (np.mean(rmse_list), np.percentile(rmse_list, [2.5, 97.5])),
        "R²":  (np.mean(r2_list), np.percentile(r2_list, [2.5, 97.5]))
    }
    return metrics

# Example dictionary with models and predictions
models = {
    "Linear Regression": y_pred_test_linear2,
    "Gradient Boosting": y_pred_test_gb2,
    "SVR": y_pred_test_svm2,
    "KNeighbors": y_pred_test_knn2,
    "Random Forest": y_pred_test_rf2,
    "MLP": y_pred_test_mlp,
    "Decision Tree": y_pred_test_dt,
    "AdaBoost": y_pred_test_ada
}

y_test_array = np.array(y_test)

# -------------------------------------------------------------------
# Colormap mais escuro: do azul-escuro (densidade baixa)
# ao vermelho-escuro (densidade alta).
# Ajuste conforme desejar (p. ex., mais ou menos vermelho).
# -------------------------------------------------------------------
colors = [
    (0.0, 0.0, 0.3),  # dark navy-like blue
    (0.6, 0.0, 0.0)   # dark red
]
dark_blue_to_red = mcolors.LinearSegmentedColormap.from_list("BlueToDarkRed", colors, N=256)

fig, axs = plt.subplots(3, 3, figsize=(24, 18))
axs = axs.flatten()

for i, (model_name, y_pred) in enumerate(models.items()):
    y_pred_array = np.array(y_pred)

    # Calculate metrics
    metrics = calculate_metrics_with_ci(y_test_array, y_pred_array)
    mae, mae_ci = metrics["MAE"]
    rmse, rmse_ci = metrics["RMSE"]
    r2, r2_ci = metrics["R²"]

    # 2D KDE for density
    xy = np.vstack([y_test_array, y_pred_array])
    kde = gaussian_kde(xy)

    x_min, x_max = y_test_array.min(), y_test_array.max()
    y_min, y_max = y_pred_array.min(), y_pred_array.max()
    overall_min = min(x_min, y_min)
    overall_max = max(x_max, y_max)

    xi = np.linspace(overall_min, overall_max, 200)
    yi = np.linspace(overall_min, overall_max, 200)
    X, Y = np.meshgrid(xi, yi)
    Z = kde(np.vstack([X.ravel(), Y.ravel()])).reshape(X.shape)

    # Contourf com alpha baixo: azul-escuro -> vermelho-escuro
    contourf = axs[i].contourf(
        X, Y, Z,
        levels=15,
        cmap=dark_blue_to_red,
        alpha=0.2  # transparência p/ ficar em segundo plano
    )

    # Halo effect (duas camadas grandes e muito transparentes)
    halo_sizes = [250, 170]
    halo_alphas = [0.05, 0.1]

    for size, alpha_val in zip(halo_sizes, halo_alphas):
        axs[i].scatter(
            y_test_array,
            y_pred_array,
            s=size,
            alpha=alpha_val,
            color='darkblue',
            edgecolors='none'
        )

    # Scatter principal: azul-escuro com contorno preto
    axs[i].scatter(
        y_test_array,
        y_pred_array,
        s=120,
        alpha=0.6,
        facecolors='darkblue',
        edgecolors='black',
        linewidths=0.7,
        label='Predicted Values'
    )

    # Linha de identidade
    axs[i].plot(
        [overall_min, overall_max],
        [overall_min, overall_max],
        'k--',
        label='y = x (Identity)'
    )

    axs[i].set_title(f"{model_name}: Actual vs Predicted", fontsize=14, pad=10)
    axs[i].set_xlabel("Actual Values", fontsize=12)
    axs[i].set_ylabel("Predicted Values", fontsize=12)
    axs[i].grid(True, alpha=0.3)
    axs[i].set_xlim([overall_min, overall_max])
    axs[i].set_ylim([overall_min, overall_max])

    # Colorbar
    cbar = fig.colorbar(contourf, ax=axs[i])
    cbar.set_label('Density', fontsize=10)

    # Metrics in the bottom-right corner
    axs[i].text(
        0.95,
        0.05,
        f"MAE: {mae:.2f} [{mae_ci[0]:.2f}, {mae_ci[1]:.2f}]\n"
        f"RMSE: {rmse:.2f} [{rmse_ci[0]:.2f}, {rmse_ci[1]:.2f}]\n"
        f"R²: {r2:.2f} [{r2_ci[0]:.2f}, {r2_ci[1]:.2f}]",
        transform=axs[i].transAxes,
        fontsize=10,
        verticalalignment='bottom',
        horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.8)
    )

    axs[i].legend(fontsize=10, loc='upper left')

# Remove extra subplots if there are fewer than 9 models
for j in range(len(models), len(axs)):
    fig.delaxes(axs[j])

plt.tight_layout()
plt.show()

# Recarregar os dados e modelos necessários
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from sklearn.utils import resample

# Previsões dos modelos
y_pred_gb = best_gb_model_cl.predict(X_test)
y_pred_lr = best_linear_model_cl.predict(X_test)
y_pred_svm = best_svm_model_cl.predict(X_test)
y_pred_knn = best_knn_model_cl.predict(X_test)
y_pred_rf = best_rf_model_cl.predict(X_test)
y_pred_mlp = best_mlp_model.predict(X_test)
y_pred_dt = best_dt_model.predict(X_test)
y_pred_ada = best_ada_model.predict(X_test)

# Cálculo do MAE para cada modelo
mae_gb = mean_absolute_error(y_test, y_pred_gb)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
mae_svm = mean_absolute_error(y_test, y_pred_svm)
mae_knn = mean_absolute_error(y_test, y_pred_knn)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mae_mlp = mean_absolute_error(y_test, y_pred_mlp)
mae_dt = mean_absolute_error(y_test, y_pred_dt)
mae_ada = mean_absolute_error(y_test, y_pred_ada)

# Função para calcular diferença de MAE com bootstrapping
def compare_mae_bootstrap(y_true, y_pred1, y_pred2, n_iterations=1000, random_state=42):
    np.random.seed(random_state)
    mae_diffs = []
    for _ in range(n_iterations):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_pred1_resampled = y_pred1[indices]
        y_pred2_resampled = y_pred2[indices]

        mae1 = mean_absolute_error(y_true_resampled, y_pred1_resampled)
        mae2 = mean_absolute_error(y_true_resampled, y_pred2_resampled)
        mae_diffs.append(mae1 - mae2)

    mean_diff = np.mean(mae_diffs)
    ci_diff = np.percentile(mae_diffs, [2.5, 97.5])

    return mean_diff, ci_diff

# Dicionário com previsões e nomes dos modelos
model_names = ["gb", "lr", "svm", "knn", "rf", "mlp", "dt", "ada"]
y_preds = {
    "gb": y_pred_gb,
    "lr": y_pred_lr,
    "svm": y_pred_svm,
    "knn": y_pred_knn,
    "rf": y_pred_rf,
    "mlp": y_pred_mlp,
    "dt": y_pred_dt,
    "ada": y_pred_ada
}

# Comparações entre os modelos
model_pairs = [(m1, m2) for i, m1 in enumerate(model_names) for m2 in model_names[i+1:]]
mean_diffs = []
cis = []

for model1, model2 in model_pairs:
    mean_diff, ci_diff = compare_mae_bootstrap(np.array(y_test), np.array(y_preds[model1]), np.array(y_preds[model2]))
    mean_diffs.append(mean_diff)
    cis.append(ci_diff)

# Criar os limites inferior e superior do IC95%
lower_bounds = np.array([ci[0] for ci in cis])
upper_bounds = np.array([ci[1] for ci in cis])

# Rótulos dos pares de modelos
model_labels = [f"{m1.upper()} vs {m2.upper()}" for m1, m2 in model_pairs]

# Criar gráfico de diferenças de MAE com IC95%
plt.figure(figsize=(16, 10))
bars = plt.bar(model_labels, mean_diffs, yerr=[np.abs(lower_bounds - np.array(mean_diffs)), np.abs(upper_bounds - np.array(mean_diffs))],
               capsize=8, color=['darkblue' if md >= 0 else 'coral' for md in mean_diffs],
               edgecolor=['black' if md >= 0 else 'darkred' for md in mean_diffs])

plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)  # Linha horizontal em y=0
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel('Model Pairs', fontsize=14)
plt.ylabel('Difference in MAE', fontsize=14)
plt.title('Difference in MAE between Model Pairs with 95% CI', fontsize=16)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Criar legenda
legend_labels = ['Positive Difference', 'Negative Difference']
legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in ['lightblue', 'lightcoral']]
plt.legend(legend_handles, legend_labels, loc='upper left', fontsize=12)

# Criar texto explicativo
legend_text = {
    "GB": "Gradient Boosting Regressor",
    "LR": "Linear Regression",
    "SVM": "Support Vector Machine Regressor",
    "KNN": "K-Nearest Neighbors Regressor",
    "RF": "Random Forest Regressor",
    "MLP": "Multi-layer Perceptron Regressor",
    "DT": "Decision Tree Regressor",
    "ADA": "AdaBoost Regressor"
}

plt.text(0.74, 0.98, "\n".join(f"{key}: {value}" for key, value in legend_text.items()),
         transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',
         bbox=dict(facecolor='white', alpha=0.8))

plt.tight_layout()
plt.savefig('Meandifference_reg.jpg', dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = df.drop('IDADE CRONOLÓGICA', axis=1)
features = X.columns

imp_features_gb = best_gb_model_cl.feature_importances_
imp_features_lr = np.abs(best_linear_model_cl.coef_.ravel())  # Garantir que é um array 1D
imp_features_dt = best_dt_model.feature_importances_
imp_features_rf = best_rf_model_cl.feature_importances_
imp_features_ada = best_ada_model.feature_importances_

sorted_indices_gb = np.argsort(imp_features_gb)
sorted_indices_lr = np.argsort(imp_features_lr)
sorted_indices_dt = np.argsort(imp_features_dt)
sorted_indices_rf = np.argsort(imp_features_rf)
sorted_indices_ada = np.argsort(imp_features_ada)

fig, axs = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Feature Importance', fontsize=16)

colors = plt.cm.viridis(np.linspace(0.2, 0.8, 5))


for ax in axs.flat:
    ax.set_facecolor('#f0f0f0')  # Fundo cinza claro
    ax.grid(True, which='both', axis='both', linestyle='--', alpha=0.7)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)

# PLOT 1: Gradient Boosting
axs[0, 0].barh(features[sorted_indices_gb], imp_features_gb[sorted_indices_gb], color=colors[0])
axs[0, 0].set_title('Gradient Boosting Regressor')
axs[0, 0].set_xlabel('Importance')
axs[0, 0].set_ylabel('Feature')

# PLOT 2: Logistic Regression
axs[0, 1].barh(features[sorted_indices_lr], imp_features_lr[sorted_indices_lr], color=colors[1])
axs[0, 1].set_title('Linear Regression')
axs[0, 1].set_xlabel('Importance')
axs[0, 1].set_ylabel('Feature')

# PLOT 3: Decision Tree
axs[0, 2].barh(features[sorted_indices_dt], imp_features_dt[sorted_indices_dt], color=colors[2])
axs[0, 2].set_title('Decision Tree Regressor')
axs[0, 2].set_xlabel('Importance')
axs[0, 2].set_ylabel('Feature')

# PLOT 4: Random Forest
axs[1, 0].barh(features[sorted_indices_rf], imp_features_rf[sorted_indices_rf], color=colors[3])
axs[1, 0].set_title('Random Forest Regressor')
axs[1, 0].set_xlabel('Importance')
axs[1, 0].set_ylabel('Feature')

# PLOT 5: AdaBoost
axs[1, 1].barh(features[sorted_indices_ada], imp_features_ada[sorted_indices_ada], color=colors[4])
axs[1, 1].set_title('AdaBoost Regressor')
axs[1, 1].set_xlabel('Importance')
axs[1, 1].set_ylabel('Feature')


fig.delaxes(axs[1, 2])

plt.tight_layout(rect=[0, 0, 1, 0.96], h_pad=1.5)
plt.savefig('FEATUREIMPORTANCE2.jpg', dpi=300, bbox_inches='tight')
plt.show()